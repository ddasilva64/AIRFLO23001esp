# Introducci√≥n a Apache Airflow

## Introducci√≥n

Airflow es una plataforma creada por la comunidad para crear, programar y supervisar flujos de trabajo de forma programada.

**_Apache Airflow_** es una plataforma de gesti√≥n de flujo de trabajo de c√≥digo abierto escrita en **_Python_**, donde los flujos de trabajo se crean a trav√©s de scripts en ese lenguaje. Fue creada por Airbnb en octubre de 2014 como soluci√≥n para la gesti√≥n de flujos de trabajo dentro de la empresa.‚Äã Al crear **_Airflow_**, Airbnb logr√≥ manejar m√°s f√°cilmente sus flujos de trabajo y controlarlos gracias a la interfaz de usuario incluida en la herramienta.‚Äã Desde el principio, el proyecto fue distribuido como c√≥digo abierto y se convirti√≥ en un proyecto de Incubadora de apache en marzo de 2016 y un proyecto de software de nivel superior de la **_Fundaci√≥n Apache_** en enero de 2019.  

**_Airflow_** est√° escrito en **_Python_** y los workflows son creados v√≠a scripts en ese lenguaje. **_Airflow_** Est√° dise√±ado bajo el principio de "configuraci√≥n como c√≥digo". Si bien hay otras plataformas de flujos de trabajo que tambi√©n trabajan bajo el principio "configuraci√≥n como c√≥digo", la mayor√≠a utiliza **_XML_**. Al utilizarse **_Python_** en **_Airflow_**, los desarrolladores pueden importar bibliotecas y clases para ayudarles crear sus flujos de trabajo.  

[Fuente Wikipedia](https://es.wikipedia.org/wiki/Apache_Airflow)

## ¬øPara qu√© sirve Airflow?

### Flujo de trabajo (**_workflow_**)

Es el estudio de los aspectos operacionales de una actividad de trabajo: c√≥mo se estructuran las tareas, c√≥mo se realizan, cu√°l es su orden correlativo, c√≥mo se sincronizan, c√≥mo fluye la informaci√≥n que soporta las tareas y c√≥mo se le hace seguimiento al cumplimiento de las tareas. Generalmente los problemas de flujo de trabajo se modelan con **_redes de Petri_**.  

Si bien el concepto de **_workflow_** no es espec√≠fico a la tecnolog√≠a de la informaci√≥n, una parte esencial del software para trabajo colaborativo (**_groupware_**) es justamente el **_workflow_**.

Una aplicaci√≥n de **_workflow_** automatiza la secuencia de acciones, actividades o tareas utilizadas para la ejecuci√≥n del proceso, incluyendo el seguimiento del estado de cada una de sus etapas y la aportaci√≥n de las herramientas necesarias para gestionarlo.

Se pueden distinguir tres tipos de actividad:

1. **Actividades colaborativas**: Un conjunto de usuarios trabajan sobre un mismo repositorio de datos para obtener un resultado com√∫n. Tiene entidad el trabajo de cada uno de ellos en s√≠ mismo.  
2. **Actividades cooperativas**: Un conjunto de usuarios trabajan sobre su propio conjunto particular, estableciendo los mecanismos de cooperaci√≥n entre ellos. No tiene entidad el trabajo de ninguno de ellos si no es visto desde el punto de vista global del resultado.  
3. **Actividades de coordinaci√≥n**.

Objetivos de un sistema de **_workflow_**:  

- Reflejar, mecanizar y **automatizar los m√©todos** y organizaci√≥n **en el sistema de informaci√≥n.**  
- Establecer los **mecanismos de control y seguimiento de los procedimientos organizativos**.  
- **Independizar** el m√©todo y **flujo de trabajo de las personas que lo ejecutan.**  
- **Facilitar la movilidad del personal**.  
- Soportar procesos de **reingenier√≠a de negocio**.  
- **Agilizar** el proceso de **intercambio de informaci√≥n y la toma de decisiones** de una organizaci√≥n, empresa o instituci√≥n.  
- Adicionalmente **optimizar el servicio**.  
- **Mejorar la gesti√≥n del conocimiento**.  

[Fuente Wikipedia](https://es.wikipedia.org/wiki/Flujo_de_trabajo)

### Canal de datos (**_data pipeline_**)

Un **_data pipeline_** es un t√©rmino utilizado para describir un **_workflow_** que consta de una o m√°s tareas que ingieren, mueven y transforman datos sin procesar desde uno o m√°s or√≠genes a un destino. Por lo general, los datos en el destino se utilizan para an√°lisis, aprendizaje autom√°tico u otras funciones comerciales. Generalmente, puede separar las canalizaciones de datos en dos categor√≠as: batch o procesamiento por lotes (el m√°s com√∫n) y canalizaciones de procesamiento en tiempo real.  

[Fuente Wiki Data Engineering](https://dataengineering.wiki/Concepts/Data+Pipeline)

<p><br></p>

![Ejemplo de uso de Airflow en la venta de paraguas](https://i.imgur.com/JnhHQS1.png)
_Ejemplo de uso de Airflow en la venta de paraguas_

<p><br></p>

![Ejemplo de uso de Airflow por equipo de Ventas y de Data](https://i.imgur.com/0kjBOLg.png)
_Ejemplo de uso de Airflow por equipo de Ventas y de Data_

<p><br></p>

[Ejemplo de uso de Airflow por equipo de Ventas y de Data -organizaci√≥n de tareas-](https://i.imgur.com/cr43MSJ.png)
_Ejemplo de uso de Airflow por equipo de Ventas y de Data -organizaci√≥n de tareas-_

<p><br></p>

**_Airflow_** organiza el **_workflow_**, pero no realiza los trabajos en s√≠. Un ejemplo simple de entender es el siguiente: si en medio de una tarea estamos procesando un **_dataframe_** de n millones de registros, esto va a ocupar mucha memoria y CPU, lo que se puede llegar a colapsar el sistema. El rendimiento en el Data es crucial por lo que tener todos los procesos en una misma m√°quina puede hacerla o muy costosa o ineficiente. Siempre es bueno distribuir las tareas complejas en diferentes m√°quinas para mejor eficiencia y reducir costos.

## ¬øPor qu√© usar Airflow?

- Open-source.
- Se puede utilizar Python.
- Interfaz sencilla.
- Muchos roles utilizan Airflow.
- > 70% de profesionales del Data entrevistados por Apache, lo usan a diario.
- Es el proyecto m√°s apoyado por desarrolladores en GitHub.
- Muchas empresas est√°n adquiriendo esta herramienta.

<p><br></p>

![Airflow es open-source](https://i.imgur.com/VbhO1bE.png)
_Airflow es open-source_

<p><br></p>

![Perfiles que acostumbran a usar Airflow](https://i.imgur.com/1M0OGbQ.png)
_Perfiles que acostumbran a usar Airflow_

<p><br></p>

![Uso promedio diario de Airflow](https://i.imgur.com/K33yFSI.png)  
_Uso promedio diario de Airflow_

<p><br></p>

![Evoluci√≥n en GitHub](https://i.imgur.com/j6zTEoI.png)
_Evoluci√≥n en GitHub_

<p><br></p>

![Empresas que usan Airflow](https://i.imgur.com/lmernOL.png)
_Empresas que usan Airflow_

<p><br></p>

En mi trabajo la evoluci√≥n ha sido:

1. Programas **_batch en Java_**.
2. **_QlikView_**.
3. **_Power BI_**.
4. **_Pentaho Data Integration (PDI)_**.

Inevitablemente, acabas llegando a **_Python_** y al **_workflow_**. He automatizado en jobs de **_PDI_** scripts en **_Python_** (de manera s√≠ncrona), pero comienzan a surgir nuevas necesidades y complejidades en esta ‚Äú_capa de la cebolla_‚Äù, como carga de procesador, etc.

Soluci√≥n para organizar el workflow: Apache Airflow, por supuesto!!! üòÉ

## Resumen

1. Desarrollar **_workflows_** con **_Python_**.
2. **_Organizar_** procesos.
3. **_Monitorizar_** ejecuciones.
4. Una **_herramienta muy usada_**.
5. Un **_job scheduler_** con esteroides (CRON ++).